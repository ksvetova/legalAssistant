{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPМодель.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "cwGNjny6erPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "cJDo971S12zP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "id": "QqtUzsPZpPG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "S73ShKJmdQpL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from catboost import CatBoostClassifier, Pool"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Глобальные параметры"
      ],
      "metadata": {
        "id": "Fb8wKhg5rau3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TAG_RANGE = list(range(1,40))\n",
        "\n",
        "TOKENIZER_DIR = ''\n",
        "TOKENIZER = TOKENIZER_DIR + \"DeepPavlov/rubert-base-cased\"\n",
        "\n",
        "MODEL_DIR = ''\n",
        "DATASET_NAME = MODEL_DIR + ''\n",
        "MODEL_NAME = MODEL_DIR + ''\n",
        "PARAMS = dict()"
      ],
      "metadata": {
        "id": "T6AhUBJirZ62"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Модели для обучения"
      ],
      "metadata": {
        "id": "R7vlwwZ4v_8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mean Pooling - Take attention mask into account for correct averaging\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "    return sum_embeddings / sum_mask\n",
        "\n",
        "class TextVectorizerBERT:\n",
        "  ''' Класс векторизации текстовых данных.'''\n",
        "\n",
        "  # Загружаем предобученныею модель токенизации текста\n",
        "  def __init__(self, tokenizer_path, model_path):\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "    self.model = AutoModel.from_pretrained(model_path)\n",
        "\n",
        "  # Приводим текст в его векторное представление\n",
        "  def vectorize(self, text):\n",
        "    encoded_input = self.tokenizer(\n",
        "        [text],\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=24,\n",
        "        return_tensors='pt')\n",
        "\n",
        "    with torch.no_grad():\n",
        "      model_output = self.model(**encoded_input)\n",
        "    \n",
        "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "\n",
        "    return sentence_embeddings[0]\n",
        "\n",
        "class ClassificationModel:\n",
        "  '''\n",
        "    Модель классификации векторных представлений текста.\n",
        "    Общий объём тегов классификации: 39\n",
        "  '''\n",
        "\n",
        "  dtset_col_txt_name = 'content'\n",
        "  dtset_col_tag_name = 'tag'\n",
        "\n",
        "  #\n",
        "  def __init__(self, vectorizer):\n",
        "    self.vectorizer = vectorizer\n",
        "\n",
        "  #\n",
        "  def load_dataset(self, path):\n",
        "    self.dataset = pd.read_csv(path,sep=';')\n",
        "\n",
        "  #\n",
        "  def vectorize_dataset(self):\n",
        "    self.vectorized_content = list(\n",
        "        map(lambda t: self.vectorizer.vectorize(t),\n",
        "            self.dataset[self.dtset_col_txt_name]))\n",
        "\n",
        "  #\n",
        "  def split_dataset(self, d_split=0.4):\n",
        "    self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(\n",
        "        self.vectorized_content,\n",
        "        self.dataset[self.dtset_col_tag_name],\n",
        "        test_size=d_split,\n",
        "        random_state=0)\n",
        "\n",
        "  #\n",
        "  def train_model(self, model_name, params = None, save_dir_path='./'):\n",
        "    self. model = CatBoostClassifier(\n",
        "        iterations=30,\n",
        "        learning_rate=0.1,\n",
        "        depth=8,\n",
        "        loss_function='TotalF1',\n",
        "        average='Macro')\n",
        "    \n",
        "    self.model.fit(\n",
        "        self.X_train,\n",
        "        self.y_train,\n",
        "        eval_set=(self.X_val, self.y_val))\n",
        "    \n",
        "    self.model.save_model(save_dir_path + model_name)\n",
        "\n",
        "  #\n",
        "  def load_model(self, model_path):\n",
        "    self.model = CatBoostClassifier()\n",
        "    self.model.load_model(model_path)\n",
        "\n",
        "  # \n",
        "  def predict_tag(self, vector):\n",
        "    prediction = self.model.predict_proba(vector)\n",
        "    highest_score = max(zip(TAG_RANGE,prediction), key=lambda pair: pair[1])\n",
        "    return highest_score\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "Wzed_4qegNf_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Парсинг документа"
      ],
      "metadata": {
        "id": "fU-kiCAgwEw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_doc(doc_path):\n",
        "  pass"
      ],
      "metadata": {
        "id": "0RChHfI2wHdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Общая инициализация"
      ],
      "metadata": {
        "id": "o4iaEybArVnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TextVectorizerBERT(TOKENIZER, TOKENIZER)\n",
        "model = ClassificationModel(vectorizer)"
      ],
      "metadata": {
        "id": "IAOwPY8DoG6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Обучение модели"
      ],
      "metadata": {
        "id": "A3O90M28oLHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_split_dataset(DATASET_NAME)\n",
        "model.train_model(MODEL_NAME,\n",
        "                  PARAMS,\n",
        "                  MODEL_DIR)"
      ],
      "metadata": {
        "id": "tidce02zoWMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Получение предсказаний модели"
      ],
      "metadata": {
        "id": "9mnUHFhpoTkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DOC_PATH = ''"
      ],
      "metadata": {
        "id": "r7GVAtnfwY8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_model(MODEL_NAME)"
      ],
      "metadata": {
        "id": "UP8iD0I-xPzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Разбиваем весь документ на абзацы, которые будем классифицировать\n",
        "texts = parse_doc(DOC_PATH)"
      ],
      "metadata": {
        "id": "T5wzaYHJxTjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finded_tags = []\n",
        "\n",
        "# Векторизуем и классифицируем извлечённый абзац документа\n",
        "for text in texts:\n",
        "  text_embedding = model.vectorizer.vectorize()\n",
        "  tag = model.predict_tag(text_embedding)\n",
        "  # Сохраняем тег, полученный при классификации\n",
        "  finded_tags.append(tag)\n",
        "\n",
        "tag_frequency = Counter(finded_tags)"
      ],
      "metadata": {
        "id": "1XYBqwIWoU0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Формирование выходных тегов"
      ],
      "metadata": {
        "id": "8TU4rufYocnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missed_tags = set(TAG_RANGE).difference(set(finded_tags)) "
      ],
      "metadata": {
        "id": "eKbG_RXaohxZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}