{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClassifierModel.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "cwGNjny6erPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "cJDo971S12zP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "id": "QqtUzsPZpPG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx "
      ],
      "metadata": {
        "id": "35db9WbMR_P1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "S73ShKJmdQpL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "import ast\n",
        "from docx import Document"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Глобальные параметры"
      ],
      "metadata": {
        "id": "Fb8wKhg5rau3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TAG_RANGE = list(range(1,40))\n",
        "\n",
        "TOKENIZER_DIR = ''\n",
        "TOKENIZER = TOKENIZER_DIR + \"DeepPavlov/rubert-base-cased\"\n",
        "\n",
        "MODEL_DIR = '/content/drive/MyDrive/'\n",
        "DATASET_NAME = MODEL_DIR + 'dataset (1).csv'\n",
        "MODEL_NAME = MODEL_DIR + 'test_model'"
      ],
      "metadata": {
        "id": "T6AhUBJirZ62"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Модели для обучения"
      ],
      "metadata": {
        "id": "R7vlwwZ4v_8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mean Pooling - Take attention mask into account for correct averaging\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "    return sum_embeddings / sum_mask\n",
        "\n",
        "class TextVectorizerBERT:\n",
        "  ''' Класс векторизации текстовых данных.'''\n",
        "\n",
        "  # Загружаем предобученныею модель токенизации текста\n",
        "  def __init__(self, tokenizer_path, model_path):\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "    self.model = AutoModel.from_pretrained(model_path)\n",
        "\n",
        "  # Приводим текст в его векторное представление\n",
        "  def vectorize(self, text):\n",
        "    encoded_input = self.tokenizer(\n",
        "        [text],\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=24,\n",
        "        return_tensors='pt')\n",
        "\n",
        "    with torch.no_grad():\n",
        "      model_output = self.model(**encoded_input)\n",
        "    \n",
        "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "    return sentence_embeddings[0].tolist()\n",
        "\n",
        "class ClassificationModel:\n",
        "  '''\n",
        "    Модель классификации векторных представлений текста.\n",
        "    Общий объём тегов классификации: 39\n",
        "  '''\n",
        "\n",
        "  dtset_col_txt_name = 'content'\n",
        "  dtset_col_tag_name = 'tag'\n",
        "  dtset_col_vec_name = 'vec_content'\n",
        "\n",
        "  #Инициализируем векторизатор и классификатор\n",
        "  def __init__(self, vectorizer):\n",
        "    self.vectorizer = vectorizer\n",
        "\n",
        "    self.model = CatBoostClassifier(\n",
        "        iterations=30,\n",
        "        loss_function='MultiClass',\n",
        "        learning_rate=0.1,\n",
        "        depth=8,\n",
        "        eval_metric='TotalF1:average=Macro')\n",
        "\n",
        "  #Загружаем датасет для обучения модели-классификатора\n",
        "  def load_dataset(self, path):\n",
        "    self.dataset = pd.read_csv(path, sep=',').iloc[:1000,:]\n",
        "    self.dataset[self.dtset_col_tag_name] = self.dataset[self.dtset_col_tag_name].astype('int32')\n",
        "\n",
        "    if self.dtset_col_vec_name in self.dataset.columns:\n",
        "      self.dataset[self.dtset_col_vec_name] = self.dataset[self.dtset_col_vec_name].apply(lambda vec: ast.literal_eval(vec))\n",
        "\n",
        "    counter = Counter(self.dataset[model.dtset_col_tag_name])\n",
        "    print(\"Частота классов:\")\n",
        "    for k,v in counter.items():\n",
        "      print(f\"{k}: {v}\")\n",
        "\n",
        "  #Получение векторных представлений текста \n",
        "  def vectorize_dataset(self):\n",
        "    if self.dtset_col_vec_name not in self.dataset.columns:\n",
        "      self.vectorized_content = list()\n",
        "      for i, text in enumerate(self.dataset[self.dtset_col_txt_name]):\n",
        "        clear_output(wait=True)\n",
        "        print(f\"{i}/{self.dataset.shape[0]}\")\n",
        "        self.vectorized_content.append(self.vectorizer.vectorize(text))\n",
        "\n",
        "      print(f\"Сохраняем векторные представления в {DATASET_NAME}\")\n",
        "      self.dataset[self.dtset_col_vec_name] = self.vectorized_content\n",
        "      self.dataset.to_csv(DATASET_NAME, sep=',')\n",
        "    else:\n",
        "      self.vectorized_content = self.dataset[self.dtset_col_vec_name]\n",
        "    \n",
        "\n",
        "  #Разбиение датасета на тренировочную и тестовую выборки\n",
        "  def split_dataset(self, d_split=0.4):\n",
        "    self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(\n",
        "        self.vectorized_content,\n",
        "        self.dataset[self.dtset_col_tag_name],\n",
        "        test_size=d_split,\n",
        "        random_state=0,\n",
        "        stratify=self.dataset[self.dtset_col_tag_name])\n",
        "\n",
        "  #Обучение модели-классификатора и сохранение её в файл\n",
        "  def train_model(self, model_name):\n",
        "    self.model.fit(\n",
        "        self.X_train,\n",
        "        self.y_train,\n",
        "        eval_set=(self.X_val, self.y_val))\n",
        "    \n",
        "    self.model.save_model(model_name)\n",
        "\n",
        "  #Загрузка предобученной модели классификатора\n",
        "  def load_model(self, model_path):\n",
        "    self.model = CatBoostClassifier()\n",
        "    self.model.load_model(model_path)\n",
        "\n",
        "  #Получение предсказания для заданного векторного представления текста.\n",
        "  #Формат: (предсказанный класс, точность предсказания)\n",
        "  def predict_tag(self, vector):\n",
        "    prediction = self.model.predict_proba(vector)\n",
        "    highest_score = max(zip(TAG_RANGE,prediction), key=lambda pair: pair[1])\n",
        "    return highest_score\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "Wzed_4qegNf_"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Парсинг документа"
      ],
      "metadata": {
        "id": "fU-kiCAgwEw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_doc(doc_path):\n",
        "  pass"
      ],
      "metadata": {
        "id": "0RChHfI2wHdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DOC_NAME = MODEL_DIR + '208_ot_18_fevralya_2022.docx'"
      ],
      "metadata": {
        "id": "RWqeZFv3Rsog"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document = Document(DOC_NAME)\n",
        "#print(document.paragraphs)"
      ],
      "metadata": {
        "id": "laNdtn2xR54d"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for p in document.paragraphs:\n",
        "    print(p.text)"
      ],
      "metadata": {
        "id": "zvl4hXL0ShD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Общая инициализация"
      ],
      "metadata": {
        "id": "o4iaEybArVnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TextVectorizerBERT(TOKENIZER, TOKENIZER)"
      ],
      "metadata": {
        "id": "jfAObbhGJdIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ClassificationModel(vectorizer)"
      ],
      "metadata": {
        "id": "IAOwPY8DoG6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Обучение модели"
      ],
      "metadata": {
        "id": "A3O90M28oLHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_dataset(DATASET_NAME)"
      ],
      "metadata": {
        "id": "tidce02zoWMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.vectorize_dataset()"
      ],
      "metadata": {
        "id": "w_dxNtvf5j32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.split_dataset()"
      ],
      "metadata": {
        "id": "0o6qRLEM5tce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train_model(MODEL_NAME)"
      ],
      "metadata": {
        "id": "_vtqQNdg42t2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Получение предсказаний модели"
      ],
      "metadata": {
        "id": "9mnUHFhpoTkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DOC_PATH = ''"
      ],
      "metadata": {
        "id": "r7GVAtnfwY8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_model(MODEL_NAME)"
      ],
      "metadata": {
        "id": "UP8iD0I-xPzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Разбиваем весь документ на абзацы, которые будем классифицировать\n",
        "texts = parse_doc(DOC_PATH)"
      ],
      "metadata": {
        "id": "T5wzaYHJxTjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finded_tags = []\n",
        "\n",
        "# Векторизуем и классифицируем извлечённый абзац документа\n",
        "for text in texts:\n",
        "  text_embedding = model.vectorizer.vectorize()\n",
        "  tag = model.predict_tag(text_embedding)\n",
        "  # Сохраняем тег, полученный при классификации\n",
        "  finded_tags.append(tag)\n",
        "\n",
        "tag_frequency = Counter(finded_tags)"
      ],
      "metadata": {
        "id": "1XYBqwIWoU0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Формирование выходных тегов"
      ],
      "metadata": {
        "id": "8TU4rufYocnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missed_tags = set(TAG_RANGE).difference(set(finded_tags)) "
      ],
      "metadata": {
        "id": "eKbG_RXaohxZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}