{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cwGNjny6erPx"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJDo971S12zP"
   },
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QqtUzsPZpPG7"
   },
   "outputs": [],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "EgFi3gVUHvD_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting docx2txt\n",
      "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
      "Building wheels for collected packages: docx2txt\n",
      "  Building wheel for docx2txt (setup.py): started\n",
      "  Building wheel for docx2txt (setup.py): finished with status 'done'\n",
      "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3980 sha256=415e10a6a36e976327442a40476b437b52cc9c9ca065d0cb14b048ddd93a1afd\n",
      "  Stored in directory: c:\\users\\mensh\\appdata\\local\\pip\\cache\\wheels\\40\\75\\01\\e6c444034338bde9c7947d3467807f889123465c2371e77418\n",
      "Successfully built docx2txt\n",
      "Installing collected packages: docx2txt\n",
      "Successfully installed docx2txt-0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "S73ShKJmdQpL"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from collections import Counter\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "import ast\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import docx2txt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mensh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fb8wKhg5rau3"
   },
   "source": [
    "Глобальные параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "T6AhUBJirZ62"
   },
   "outputs": [],
   "source": [
    "TAG_RANGE = list(range(1,40))\n",
    "\n",
    "TOKENIZER_DIR = ''\n",
    "TOKENIZER = TOKENIZER_DIR + \"DeepPavlov/rubert-base-cased\"\n",
    "\n",
    "MODEL_DIR = './datasets/'\n",
    "DATASET_NAME = MODEL_DIR + 'dataset4.csv'\n",
    "MODEL_NAME = MODEL_DIR + 'pretrained_multiclass_model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7vlwwZ4v_8e"
   },
   "source": [
    "# Модели для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Wzed_4qegNf_"
   },
   "outputs": [],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "class TextVectorizerBERT:\n",
    "    ''' Класс векторизации текстовых данных.'''\n",
    "\n",
    "  # Загружаем предобученныею модель токенизации текста\n",
    "    def __init__(self, tokenizer_path, model_path):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "        self.model = AutoModel.from_pretrained(model_path)\n",
    "\n",
    "  # Приводим текст в его векторное представление\n",
    "    def vectorize(self, text):\n",
    "        encoded_input = self.tokenizer(\n",
    "            [text],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=24,\n",
    "            return_tensors='pt')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "        \n",
    "        sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "        return sentence_embeddings[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel:\n",
    "    '''\n",
    "    Модель классификации векторных представлений текста.\n",
    "    Общий объём тегов классификации: 39\n",
    "    '''\n",
    "    \n",
    "    dtset_col_txt_name = 'content'\n",
    "    dtset_col_tag_name = 'tag'\n",
    "    dtset_col_vec_name = 'vec_content'\n",
    "\n",
    "    #Инициализируем векторизатор и классификатор\n",
    "    def __init__(self, vectorizer):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.model = CatBoostClassifier(\n",
    "          iterations=30,\n",
    "          loss_function='MultiClass',\n",
    "          learning_rate=0.1,\n",
    "          depth=8,\n",
    "          eval_metric='TotalF1:average=Macro')\n",
    "\n",
    "    #Загружаем датасет для обучения модели-классификатора\n",
    "    def load_dataset(self, path):\n",
    "        self.dataset = pd.read_csv(path, sep=',')\n",
    "        print(f\"Drop nan. shape before {self.dataset.shape}.\")\n",
    "        self.dataset = self.dataset.dropna()\n",
    "        print(f\"Drop nan. shape after {self.dataset.shape}\")\n",
    "\n",
    "        self.dataset[self.dtset_col_tag_name] = self.dataset[self.dtset_col_tag_name].astype('int32')\n",
    "\n",
    "        if self.dtset_col_vec_name in self.dataset.columns:\n",
    "            self.dataset[self.dtset_col_vec_name] = self.dataset[self.dtset_col_vec_name].apply(lambda vec: ast.literal_eval(vec))\n",
    "\n",
    "        counter = Counter(self.dataset[model.dtset_col_tag_name])\n",
    "        print(\"Частота классов:\")\n",
    "        for k,v in counter.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "\n",
    "    #Получение векторных представлений текста \n",
    "    def vectorize_dataset(self, dataset_name):\n",
    "        if self.dtset_col_vec_name not in self.dataset.columns:\n",
    "            self.vectorized_content = list()\n",
    "            for i, text in enumerate(self.dataset[self.dtset_col_txt_name]):\n",
    "                clear_output(wait=True)\n",
    "                print(f\"{i}/{self.dataset.shape[0]}\")\n",
    "                self.vectorized_content.append(self.vectorizer.vectorize(text))\n",
    "            \n",
    "            print(f\"Сохраняем векторные представления в {dataset_name}\")\n",
    "            self.dataset[self.dtset_col_vec_name] = self.vectorized_content\n",
    "            self.dataset.to_csv(dataset_name, sep=',', index=False)\n",
    "        else:\n",
    "            self.vectorized_content = self.dataset[self.dtset_col_vec_name]\n",
    "    \n",
    "\n",
    "    #Разбиение датасета на тренировочную и тестовую выборки\n",
    "    def split_dataset(self, d_split=0.4):\n",
    "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(\n",
    "            self.vectorized_content,\n",
    "            self.dataset[self.dtset_col_tag_name],\n",
    "            test_size=d_split,\n",
    "            random_state=0,\n",
    "            stratify=self.dataset[self.dtset_col_tag_name])\n",
    "\n",
    "    #Обучение модели-классификатора и сохранение её в файл\n",
    "    def train_model(self, model_name):\n",
    "        self.model.fit(\n",
    "            self.X_train,\n",
    "            self.y_train,\n",
    "            eval_set=(self.X_val, self.y_val),\n",
    "            plot=True)\n",
    "    \n",
    "        self.model.save_model(model_name)\n",
    "\n",
    "    #Загрузка предобученной модели классификатора\n",
    "    def load_model(self, model_path):\n",
    "        self.model = CatBoostClassifier()\n",
    "        self.model.load_model(model_path)\n",
    "\n",
    "    #Получение предсказания для заданного векторного представления текста.\n",
    "    #Формат: (предсказанный класс, точность предсказания)\n",
    "    def predict_tag(self, vector):\n",
    "        prediction = self.model.predict_proba(vector)\n",
    "        highest_score = max(zip(TAG_RANGE,prediction), key=lambda pair: pair[1])\n",
    "        return highest_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fU-kiCAgwEw9"
   },
   "source": [
    "# Парсинг документа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0RChHfI2wHdc"
   },
   "outputs": [],
   "source": [
    "def parse_doc(doc_path):\n",
    "    raw_text = docx2txt.process(doc_path)\n",
    "  \n",
    "    # Удаляем шапку документа\n",
    "    ignorecase_head_ptr = re.compile('\\n{2,}утверждены\\n{2,}постановлением правительства\\n{2,}Российской Федерации\\n{2,}', re.IGNORECASE)\n",
    "    pos = ignorecase_head_ptr.search(raw_text).span()[1]\n",
    "    raw_text = raw_text[pos:]\n",
    "\n",
    "    print('here1')\n",
    "\n",
    "    # Удаляем дополнительные Приложения в хвосте документа\n",
    "    tail_ptr1 = re.compile('\\n{3,}приложение( \\w+){,2}\\n{2}', re.IGNORECASE)\n",
    "    ptr = tail_ptr1.search(raw_text)\n",
    "    if ptr:\n",
    "        pos = ptr.span()[0]\n",
    "        raw_text = raw_text[:pos]\n",
    "\n",
    "    print('here2')\n",
    "  \n",
    "    # Удаляем новые заголовки с текстом в хвосте документа\n",
    "    tail_ptr2 = re.compile('(([А-Я,\"]+[ ]*)+\\n{2,}){3,}')\n",
    "    title_span = tail_ptr2.search(raw_text).span()\n",
    "  \n",
    "    print('here3')\n",
    "\n",
    "    ptr = tail_ptr2.search(raw_text[title_span[1]:])\n",
    "    if ptr:\n",
    "        pos = ptr.span()[0]\n",
    "        raw_text = raw_text[pos+title_span[1]:]\n",
    "    raw_text = raw_text[title_span[1]:]\n",
    "\n",
    "    # Удаляем мусор из анализируемой части документа\n",
    "    parts = list(filter(lambda v: v and 'www.consultant.ru' not in v \n",
    "                        and 'Список изменяющих документов\\n\\n(в ред. Постановлений Правительства РФ' not in v\n",
    "                        and v not in [' ', '  '],\n",
    "                        re.split('\\n{5,}', raw_text)))\n",
    "  \n",
    "    raw_text = '\\n\\n'.join(map(lambda v: v.strip('\\n'), parts))\n",
    "    raw_text = re.sub('\\n{4} \\n{4}', ' ', raw_text)\n",
    "\n",
    "    # Разбиваем текст на абзацы\n",
    "    parts = list(filter(lambda v: v != ' ' and v, re.split('\\n\\n', raw_text)))\n",
    "   \n",
    "    # Разбиваем абзацы на предложения\n",
    "    sents = []\n",
    "    for part in parts:\n",
    "        sents += sent_tokenize(part)\n",
    "    sents = list(filter(lambda v: len(v) > 5,sents))\n",
    "\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCb9xQNte3Gg"
   },
   "source": [
    "Инициализация "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "id": "jfAObbhGJdIv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TextVectorizerBERT(TOKENIZER, TOKENIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IAOwPY8DoG6K"
   },
   "outputs": [],
   "source": [
    "model = ClassificationModel(vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3O90M28oLHu"
   },
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "w_dxNtvf5j32"
   },
   "outputs": [],
   "source": [
    "#Разбиваем датасет на части (чтобы всё влезло в память при векторизации)\n",
    "PART_SIZE = 10000 \n",
    "PARTS_DIR = MODEL_DIR + 'parts_dataset_dir/'\n",
    "\n",
    "full_dataset = pd.read_csv(DATASET_NAME, sep=',')\n",
    "part_dataset_names = []\n",
    "\n",
    "start = 0\n",
    "end = 0\n",
    "rows_count = full_dataset.shape[0]\n",
    "parts = math.ceil(rows_count / PART_SIZE)\n",
    "for i in range(parts):\n",
    "    if start + PART_SIZE <= rows_count:\n",
    "        start = end\n",
    "        end = start + PART_SIZE\n",
    "    else:\n",
    "        start = end\n",
    "        end = rows_count\n",
    "        \n",
    "    new_dataset_name = f\"{PARTS_DIR}{DATASET_NAME}_part{i}.csv\"\n",
    "    full_dataset.iloc[start:end,:].to_csv(new_dataset_name, index=False)\n",
    "    part_dataset_names.append(new_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "TsmTGyG0sFAb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3762/3763\n",
      "Сохраняем векторные представления в ./parts_dataset_dir/./dataset4.csv_part6.csv\n"
     ]
    }
   ],
   "source": [
    "#Векторизуем каждый датасет\n",
    "for part_name in part_dataset_names:\n",
    "    model.load_dataset(part_name)\n",
    "    model.vectorize_dataset(part_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "K6WSxMOSsG6a"
   },
   "outputs": [],
   "source": [
    "#Объединяем векторизированные датасеты в один\n",
    "full_dataset = None\n",
    "for part_name in part_dataset_names:\n",
    "    tmp_df = pd.read_csv(part_name,sep=',')\n",
    "    if full_dataset is None:\n",
    "        full_dataset = tmp_df\n",
    "    else:\n",
    "        full_dataset = pd.concat([full_dataset, tmp_df])\n",
    "full_dataset.to_csv(DATASET_NAME, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "id": "JxYLNDljvkYB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop nan. shape before (63748, 4).\n",
      "Drop nan. shape after (63748, 4)\n",
      "Частота классов:\n",
      "2: 1635\n",
      "1: 1635\n",
      "35: 1635\n",
      "24: 1635\n",
      "32: 1635\n",
      "27: 1635\n",
      "38: 1635\n",
      "25: 1635\n",
      "31: 1635\n",
      "26: 1635\n",
      "18: 1635\n",
      "11: 1635\n",
      "19: 1635\n",
      "20: 1635\n",
      "21: 1635\n",
      "14: 1635\n",
      "7: 1635\n",
      "29: 1635\n",
      "28: 1635\n",
      "36: 1635\n",
      "37: 1635\n",
      "33: 1635\n",
      "34: 1635\n",
      "4: 1635\n",
      "3: 1623\n",
      "23: 1635\n",
      "39: 1635\n",
      "22: 1630\n",
      "8: 1635\n",
      "10: 1635\n",
      "13: 1635\n",
      "15: 1635\n",
      "6: 1635\n",
      "12: 1635\n",
      "30: 1635\n",
      "17: 1635\n",
      "5: 1635\n",
      "16: 1635\n",
      "9: 1635\n"
     ]
    }
   ],
   "source": [
    "model.load_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "id": "0o6qRLEM5tce"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ClassificationModel' object has no attribute 'vectorized_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5336/2089760228.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5336/3077836931.py\u001b[0m in \u001b[0;36msplit_dataset\u001b[1;34m(self, d_split)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msplit_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(\n\u001b[1;32m---> 57\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectorized_content\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtset_col_tag_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0md_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ClassificationModel' object has no attribute 'vectorized_content'"
     ]
    }
   ],
   "source": [
    "model.split_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vtqQNdg42t2"
   },
   "source": [
    "model.train_model(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mnUHFhpoTkn"
   },
   "source": [
    "## Получение предсказаний модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UP8iD0I-xPzE"
   },
   "outputs": [],
   "source": [
    "model.load_model(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1XYBqwIWoU0Z",
    "outputId": "0aec6283-6e70-4af2-902c-a8e09fa9de8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/105\n"
     ]
    }
   ],
   "source": [
    "def make_predictions(texts):\n",
    "  predictions = []\n",
    "  l_texts = len(texts)\n",
    "\n",
    "  # Векторизуем и классифицируем извлечённый абзац документа\n",
    "  for i, text in enumerate(texts):\n",
    "    print(f\"{i}/{l_texts}\")\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    text_embedding = model.vectorizer.vectorize(text)\n",
    "    predict = model.predict_tag(text_embedding)\n",
    "    # Сохраняем тег, полученный при классификации\n",
    "    predictions.append(predict[0])\n",
    "\n",
    "  return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3SAqD753mVB"
   },
   "source": [
    "Формирование статистики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "jj6JRpZE3vKL"
   },
   "outputs": [],
   "source": [
    "def create_statistics_json(predictions):\n",
    "    tags_list = list(map(lambda v: v[0], predictions))\n",
    "    \n",
    "    # присутствующие теги\n",
    "    present_tags = list(set(tags_list))\n",
    "    \n",
    "    # отсутствующие теги\n",
    "    missed_tags = set(TAG_RANGE).difference(set(tags_list))\n",
    "\n",
    "    # частота присутствующих признаков\n",
    "    tag_frequency = dict(Counter(tags_list))\n",
    "\n",
    "    tag_accur_median = dict()\n",
    "    tag_accur_var = dict()\n",
    "    for key, group_items in groupby(predictions, key=lambda p: p[0]):\n",
    "        accurs = list(map(lambda p: p[1],group_items))\n",
    "        # медиана точности по каждому тегу\n",
    "        tag_accur_median[key] = np.median(accurs)\n",
    "        \n",
    "    # дисперсия точности по каждому тегу    \n",
    "    tag_accur_var[key] = np.var(accurs)\n",
    "  \n",
    "\n",
    "\n",
    "    # формируем единый json-файл со всей статистикой\n",
    "    data = dict()\n",
    "    data['present tags'] = present_tags\n",
    "    data['missed tags'] = missed_tags\n",
    "    data['tags freq'] = tag_frequency\n",
    "    data['accur median'] = tag_accur_median\n",
    "    data['accur var'] = tag_accur_var\n",
    "    data['pred list'] = predictions\n",
    "\n",
    "    with open('statistics_document.json', 'w') as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVqVTaLDmwel"
   },
   "source": [
    "Формируем единый датасет для тестирования модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATASET_DIR = MODEL_DIR + './test_dataset/'\n",
    "FULL_DATASET_NAME = MODEL_DIR + 'test_dataset.csv'\n",
    "INFO_DATASET_NAME = MODEL_DIR + 'test_dataset_predictions.csv'\n",
    "\n",
    "FIELD_PRED_TAG = 'class'\n",
    "FIELD_FILE_I = 'file_i'\n",
    "FILED_ID = 'id'\n",
    "FIELD_TEXT = 'content'\n",
    "\n",
    "doc_names = list(map(lambda v: TEST_DATASET_DIR + v,os.listdir(TEST_DATASET_DIR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = pd.DataFrame(columns=[FIELD_FILE_I,FILED_ID,FIELD_TEXT])\n",
    "\n",
    "doc_file_i_s = []\n",
    "doc_ids = []\n",
    "doc_part_txt = []\n",
    "\n",
    "for i, name in enumerate(doc_names):\n",
    "  doc_id = int(name.split('.')[-2].split('/')[-1])\n",
    "\n",
    "  raw_text = docx2txt.process(name)\n",
    "  raw_text = re.sub('\\n+', ' ', raw_text)\n",
    "\n",
    "  patterns = re.findall(r'\\{\\d+\\}.*?\\{\\d+\\}', raw_text)\n",
    "\n",
    "  for j, ptr in enumerate(patterns):\n",
    "    clear_output(wait=True)\n",
    "    print(f\"{i}/{len(doc_names)} : {j}/{len(patterns)}\")\n",
    "\n",
    "    text = re.findall('\\}(.*?)\\{',ptr)[0]\n",
    "    num = int(re.findall('\\{(.*?)\\}',ptr)[0])\n",
    "\n",
    "    doc_file_i_s.append(doc_id)\n",
    "    doc_ids.append(num)\n",
    "    doc_part_txt.append(text)\n",
    "\n",
    "full_dataset[FIELD_FILE_I] = doc_file_i_s\n",
    "full_dataset[FILED_ID] = doc_ids\n",
    "full_dataset[FIELD_TEXT] = doc_part_txt\n",
    "\n",
    "full_dataset.to_csv(FULL_DATASET_NAME,index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TextVectorizerBERT(TOKENIZER, TOKENIZER)\n",
    "model = ClassificationModel(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_dataset(FULL_DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = make_predictions(model.dataset[model.dtset_col_txt_name])\n",
    "model.dataset[FIELD_PRED_TAG] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info = model.dataset\n",
    "df_info =df_info.drop(columns=['vec_content','content'])\n",
    "df_info.to_csv(INFO_DATASET_NAME, index=False,sep=',')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ClassifierModel.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
