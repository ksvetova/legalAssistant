{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClassifierModel.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "cwGNjny6erPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "cJDo971S12zP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "id": "QqtUzsPZpPG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docx2txt"
      ],
      "metadata": {
        "id": "EgFi3gVUHvD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "ZRQ_eo0Nzvoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "S73ShKJmdQpL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "import ast\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import docx2txt\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Глобальные параметры"
      ],
      "metadata": {
        "id": "Fb8wKhg5rau3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TAG_RANGE = list(range(1,40))\n",
        "\n",
        "TOKENIZER_DIR = ''\n",
        "TOKENIZER = TOKENIZER_DIR + \"DeepPavlov/rubert-base-cased\"\n",
        "\n",
        "MODEL_DIR = '/content/drive/MyDrive/'\n",
        "DATASET_NAME = MODEL_DIR + 'dataset4.csv'\n",
        "MODEL_NAME = MODEL_DIR + 'test_model'"
      ],
      "metadata": {
        "id": "T6AhUBJirZ62"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Модели для обучения"
      ],
      "metadata": {
        "id": "R7vlwwZ4v_8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mean Pooling - Take attention mask into account for correct averaging\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "    return sum_embeddings / sum_mask\n",
        "\n",
        "class TextVectorizerBERT:\n",
        "  ''' Класс векторизации текстовых данных.'''\n",
        "\n",
        "  # Загружаем предобученныею модель токенизации текста\n",
        "  def __init__(self, tokenizer_path, model_path):\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "    self.model = AutoModel.from_pretrained(model_path)\n",
        "\n",
        "  # Приводим текст в его векторное представление\n",
        "  def vectorize(self, text):\n",
        "    encoded_input = self.tokenizer(\n",
        "        [text],\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=24,\n",
        "        return_tensors='pt')\n",
        "\n",
        "    with torch.no_grad():\n",
        "      model_output = self.model(**encoded_input)\n",
        "    \n",
        "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "    return sentence_embeddings[0].tolist()\n",
        "\n",
        "class ClassificationModel:\n",
        "  '''\n",
        "    Модель классификации векторных представлений текста.\n",
        "    Общий объём тегов классификации: 39\n",
        "  '''\n",
        "\n",
        "  dtset_col_txt_name = 'content'\n",
        "  dtset_col_tag_name = 'tag'\n",
        "  dtset_col_vec_name = 'vec_content'\n",
        "\n",
        "  #Инициализируем векторизатор и классификатор\n",
        "  def __init__(self, vectorizer):\n",
        "    self.vectorizer = vectorizer\n",
        "\n",
        "    self.model = CatBoostClassifier(\n",
        "        iterations=30,\n",
        "        loss_function='MultiClass',\n",
        "        learning_rate=0.1,\n",
        "        depth=8,\n",
        "        eval_metric='TotalF1:average=Macro')\n",
        "\n",
        "  #Загружаем датасет для обучения модели-классификатора\n",
        "  def load_dataset(self, path):\n",
        "    self.dataset = pd.read_csv(path, sep=',')\n",
        "    print(f\"Drop nan. shape before {self.dataset.shape}.\")\n",
        "    self.dataset = self.dataset.dropna()\n",
        "    print(f\"Drop nan. shape after {self.dataset.shape}\")\n",
        "\n",
        "    self.dataset[self.dtset_col_tag_name] = self.dataset[self.dtset_col_tag_name].astype('int32')\n",
        "\n",
        "    if self.dtset_col_vec_name in self.dataset.columns:\n",
        "      self.dataset[self.dtset_col_vec_name] = self.dataset[self.dtset_col_vec_name].apply(lambda vec: ast.literal_eval(vec))\n",
        "\n",
        "    counter = Counter(self.dataset[model.dtset_col_tag_name])\n",
        "    print(\"Частота классов:\")\n",
        "    for k,v in counter.items():\n",
        "      print(f\"{k}: {v}\")\n",
        "\n",
        "  #Получение векторных представлений текста \n",
        "  def vectorize_dataset(self):\n",
        "    if self.dtset_col_vec_name not in self.dataset.columns:\n",
        "      self.vectorized_content = list()\n",
        "      for i, text in enumerate(self.dataset[self.dtset_col_txt_name]):\n",
        "        clear_output(wait=True)\n",
        "        print(f\"{i}/{self.dataset.shape[0]}\")\n",
        "        self.vectorized_content.append(self.vectorizer.vectorize(text))\n",
        "\n",
        "      print(f\"Сохраняем векторные представления в {DATASET_NAME}\")\n",
        "      self.dataset[self.dtset_col_vec_name] = self.vectorized_content\n",
        "      self.dataset.to_csv(DATASET_NAME, sep=',')\n",
        "    else:\n",
        "      self.vectorized_content = self.dataset[self.dtset_col_vec_name]\n",
        "    \n",
        "\n",
        "  #Разбиение датасета на тренировочную и тестовую выборки\n",
        "  def split_dataset(self, d_split=0.4):\n",
        "    self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(\n",
        "        self.vectorized_content,\n",
        "        self.dataset[self.dtset_col_tag_name],\n",
        "        test_size=d_split,\n",
        "        random_state=0,\n",
        "        stratify=self.dataset[self.dtset_col_tag_name])\n",
        "\n",
        "  #Обучение модели-классификатора и сохранение её в файл\n",
        "  def train_model(self, model_name):\n",
        "    self.model.fit(\n",
        "        self.X_train,\n",
        "        self.y_train,\n",
        "        eval_set=(self.X_val, self.y_val),\n",
        "        plot=True)\n",
        "    \n",
        "    self.model.save_model(model_name)\n",
        "\n",
        "  #Загрузка предобученной модели классификатора\n",
        "  def load_model(self, model_path):\n",
        "    self.model = CatBoostClassifier()\n",
        "    self.model.load_model(model_path)\n",
        "\n",
        "  #Получение предсказания для заданного векторного представления текста.\n",
        "  #Формат: (предсказанный класс, точность предсказания)\n",
        "  def predict_tag(self, vector):\n",
        "    prediction = self.model.predict_proba(vector)\n",
        "    highest_score = max(zip(TAG_RANGE,prediction), key=lambda pair: pair[1])\n",
        "    return highest_score\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "Wzed_4qegNf_"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(zip([1,2],[3,4]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihVAXjoy4SSu",
        "outputId": "f39b9393-7248-4900-b057-bd6ce907ad1f"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 3), (2, 4)]"
            ]
          },
          "metadata": {},
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Парсинг документа"
      ],
      "metadata": {
        "id": "fU-kiCAgwEw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_doc(doc_path):\n",
        "  raw_text = docx2txt.process(doc_path)\n",
        "  \n",
        "  # Удаляем шапку документа\n",
        "  ignorecase_head_ptr = re.compile('\\n{2,}утверждены\\n{2,}постановлением правительства\\n{2,}Российской Федерации\\n{2,}', re.IGNORECASE)\n",
        "  pos = ignorecase_head_ptr.search(raw_text).span()[1]\n",
        "  raw_text = raw_text[pos:]\n",
        "\n",
        "  print('here1')\n",
        "\n",
        "  # Удаляем дополнительные Приложения в хвосте документа\n",
        "  tail_ptr1 = re.compile('\\n{3,}приложение( \\w+){,2}\\n{2}', re.IGNORECASE)\n",
        "  ptr = tail_ptr1.search(raw_text)\n",
        "  if ptr:\n",
        "    pos = ptr.span()[0]\n",
        "    raw_text = raw_text[:pos]\n",
        "\n",
        "  print('here2')\n",
        "  \n",
        "  # Удаляем новые заголовки с текстом в хвосте документа\n",
        "  tail_ptr2 = re.compile('(([А-Я,\"]+[ ]*)+\\n{2,}){3,}')\n",
        "  title_span = tail_ptr2.search(raw_text).span()\n",
        "  \n",
        "  print('here3')\n",
        "\n",
        "  ptr = tail_ptr2.search(raw_text[title_span[1]:])\n",
        "  if ptr:\n",
        "    pos = ptr.span()[0]\n",
        "    raw_text = raw_text[pos+title_span[1]:]\n",
        "  raw_text = raw_text[title_span[1]:]\n",
        "\n",
        "  # Удаляем мусор из анализируемой части документа\n",
        "  parts = list(filter(lambda v: v and 'www.consultant.ru' not in v \n",
        "                      and 'Список изменяющих документов\\n\\n(в ред. Постановлений Правительства РФ' not in v\n",
        "                      and v not in [' ', '  '],\n",
        "                 re.split('\\n{5,}', raw_text)))\n",
        "  \n",
        "  raw_text = '\\n\\n'.join(map(lambda v: v.strip('\\n'), parts))\n",
        "  raw_text = re.sub('\\n{4} \\n{4}', ' ', raw_text)\n",
        "\n",
        "  # Разбиваем текст на абзацы\n",
        "  parts = list(filter(lambda v: v != ' ' and v, re.split('\\n\\n', raw_text)))\n",
        "   \n",
        "  # Разбиваем абзацы на предложения\n",
        "  sents = []\n",
        "  for part in parts:\n",
        "    sents += sent_tokenize(part)\n",
        "  sents = list(filter(lambda v: len(v) > 5,sents))\n",
        "\n",
        "  return sents"
      ],
      "metadata": {
        "id": "0RChHfI2wHdc"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Инициализация "
      ],
      "metadata": {
        "id": "CCb9xQNte3Gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TextVectorizerBERT(TOKENIZER, TOKENIZER)"
      ],
      "metadata": {
        "id": "jfAObbhGJdIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ClassificationModel(vectorizer)"
      ],
      "metadata": {
        "id": "IAOwPY8DoG6K"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Обучение модели"
      ],
      "metadata": {
        "id": "A3O90M28oLHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_dataset(DATASET_NAME)"
      ],
      "metadata": {
        "id": "tidce02zoWMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.vectorize_dataset()"
      ],
      "metadata": {
        "id": "w_dxNtvf5j32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.dataset.iloc[2412,:]"
      ],
      "metadata": {
        "id": "JxYLNDljvkYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.split_dataset()"
      ],
      "metadata": {
        "id": "0o6qRLEM5tce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(model.X_train)"
      ],
      "metadata": {
        "id": "MhU6OlGhyuQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train_model(MODEL_NAME)"
      ],
      "metadata": {
        "id": "_vtqQNdg42t2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Получение предсказаний модели"
      ],
      "metadata": {
        "id": "9mnUHFhpoTkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DOC_NAME = MODEL_DIR + \"Postanovlenie_Pravitelstva_RF_ot_13_03_2021_N_369.docx\""
      ],
      "metadata": {
        "id": "r7GVAtnfwY8O"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_model(MODEL_NAME)"
      ],
      "metadata": {
        "id": "UP8iD0I-xPzE"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Разбиваем весь документ на предложения, которые будем классифицировать\n",
        "texts = parse_doc(DOC_NAME)\n",
        "l_texts = len(texts)"
      ],
      "metadata": {
        "id": "T5wzaYHJxTjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "\n",
        "# Векторизуем и классифицируем извлечённый абзац документа\n",
        "for i, text in enumerate(texts):\n",
        "  print(f\"{i}/{l_texts}\")\n",
        "  clear_output(wait=True)\n",
        "\n",
        "  text_embedding = model.vectorizer.vectorize(text)\n",
        "  predict = model.predict_tag(text_embedding)\n",
        "  # Сохраняем тег, полученный при классификации\n",
        "  predictions.append(predict)"
      ],
      "metadata": {
        "id": "1XYBqwIWoU0Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aec6283-6e70-4af2-902c-a8e09fa9de8a"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "104/105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Формирование статистики"
      ],
      "metadata": {
        "id": "D3SAqD753mVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_statistics_json(predictions):\n",
        "  tags_list = list(map(lambda v: v[0], predictions))\n",
        "  tag_frequency = Counter(tags_list)\n",
        "  missed_tags = set(TAG_RANGE).difference(set(tags_list)) "
      ],
      "metadata": {
        "id": "jj6JRpZE3vKL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}